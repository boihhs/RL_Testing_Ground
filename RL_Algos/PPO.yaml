PPO:
  mini_batch_size: 256
  
  gamma: 0.995
  lambda: .95
  e_clip: 0.4
  learning_rate_value: 1e-3
  learning_rate_policy: 1e-3
  batch_size: 1024
  bound_coef: 1
  entropy_coef:  -.1
  
  mini_batch_loops: 10
  num_epocs: 5000
  max_timesteps: 300
  horizon_length: 25

  xml_path: "./Robot_Models/booster_t1/scene.xml"
  model_freq: 50
  std_gyro: 0
  std_acc: 0
  std_joint_pos: 0
  std_joint_vel: 0

  policy_model_shape: [91, 256, 128, 128, 46] # action_dim * 2
  value_model_shape: [91, 256, 256, 128, 1]
  action_scale: [1.57, 0.785, 2.265, 1.655, 2.27, 1.22,
    2.265, 1.655, 2.27, 1.22, 1.57,
    1.685, 0.885, 1.0, 1.17, 0.61, 0.44,
    1.685, 0.885, 1.0, 1.17, 0.61, 0.44]
  action_bias: [0.00,  0.435, -1.045, -0.085,  0.00, -1.220,
    -1.045,  0.085,  0.00,  1.220,  0.00,
    -0.115,  0.685,  0.00,  1.170, -0.260,  0.00,
    -0.115, -0.685,  0.00,  1.170, -0.260,  0.00]

  policy_state_dim: 91
  value_state_dim: 91
  total_obs_dim: 91
  
  action_dim: 23

  init_pos: [0., 0., 0.015,
    0., 0.7071068, 0., 0.7071068,
    0, 0,
    0.2, -1.35, 0, -0.5,
    0.2, 1.35, 0, 0.5,
    0,
    -0.2, 0, 0, 0.4, -0.25, 0,
    -0.2, 0, 0, 0.4, -0.25, 0]

  init_vel: [0, 0, 0,
    0., 0., 0.,
    0., 0.,
    0., 0., 0., 0.,
    0., 0., 0., 0.,
    0.,
    0., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0.]

  stiffness: [
    20, 20,
    20, 20, 20, 20,
    20, 20, 20, 20,
    200,
    200, 200, 200, 200, 50, 50,
    200, 200, 200, 200, 50, 50
  ]
  damping: [
    0.2, 0.2,
    0.5, 0.5, 0.5, 0.5,
    0.5, 0.5, 0.5, 0.5,
    5,
    5, 5, 5, 5, 3, 3,
    5, 5, 5, 5, 3, 3
  ]
  
  torque_limit: [
    7, 7,
    10, 10, 10, 10,
    10, 10, 10, 10,
    30,
    60, 25, 30, 60, 24, 15,
    60, 25, 30, 60, 24, 15
  ]

  default_qpos: [
    0, 0,
    0.2, -1.35, 0, -0.5,
    0.2, 1.35, 0, 0.5,
    0,
    -0.2, 0, 0, 0.4, -0.25, 0,
    -0.2, 0, 0, 0.4, -0.25, 0
  ]

Rewards:

  forward_velocity: 5
  effort: .001
  stablity_ang: .001
  smooth_lin: .001
  height: 10
  desired_height: .6